{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO33E0qSY1NU2bpgX4XjDjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/code-with-shannu/Pytorch/blob/main/logistic_regression_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0VsyrcoxgcRN"
      },
      "outputs": [],
      "source": [
        "#import all libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 0: create Dataset\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data,bc.target\n",
        "\n",
        "#print(X.shape)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "#Split our data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1234)\n",
        "\n",
        "#Scale our data\n",
        "sc = StandardScaler()# this will set our features to zero mean and unit variance\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "#convert numpy ndarray to tensor\n",
        "X_train=torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test =torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train=torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test =torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "#reshape the data\n",
        "#print(y_train)\n",
        "y_train = y_train.view(y_train.shape[0],1)  #right now we have only one row in y so we are converting it into column vector\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "#print(y_train)\n",
        "\n",
        "#### STEP 1: Create model(input_features, otput,forward_pass)   y = wx+b and in logistic regression we apply sigmoid function at the end\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self,n_input_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "#### step 2: construct loss and optimizer\n",
        "criterion = nn.BCELoss()           #binary cross entropy loss\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
        "\n",
        "\n",
        "# step 3: Training loop:\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "#         --forward pass, prediction and loss\n",
        "  y_predicted = model.forward(X_train)\n",
        "  loss = criterion(y_predicted,y_train)\n",
        "\n",
        "#         --backward pass and gradients\n",
        "  loss.backward()\n",
        "#         --update weights\n",
        "  optimizer.step()\n",
        "#set our gradients zero\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch%10==0:\n",
        "    print(f'epoch:{epoch+1}, loss:{loss.item():.4f}')\n",
        "with torch.no_grad():\n",
        "    y_predicted = model(X_test)\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'accuracy: {acc.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qEZ3xXaVnVs",
        "outputId": "f92ba0b8-c6b9-43c4-95c7-310e883a0776"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, loss:0.8682\n",
            "epoch:11, loss:0.6453\n",
            "epoch:21, loss:0.5183\n",
            "epoch:31, loss:0.4409\n",
            "epoch:41, loss:0.3891\n",
            "epoch:51, loss:0.3517\n",
            "epoch:61, loss:0.3231\n",
            "epoch:71, loss:0.3004\n",
            "epoch:81, loss:0.2818\n",
            "epoch:91, loss:0.2663\n",
            "epoch:101, loss:0.2530\n",
            "epoch:111, loss:0.2415\n",
            "epoch:121, loss:0.2314\n",
            "epoch:131, loss:0.2225\n",
            "epoch:141, loss:0.2145\n",
            "epoch:151, loss:0.2073\n",
            "epoch:161, loss:0.2008\n",
            "epoch:171, loss:0.1949\n",
            "epoch:181, loss:0.1895\n",
            "epoch:191, loss:0.1845\n",
            "epoch:201, loss:0.1798\n",
            "epoch:211, loss:0.1755\n",
            "epoch:221, loss:0.1715\n",
            "epoch:231, loss:0.1678\n",
            "epoch:241, loss:0.1643\n",
            "epoch:251, loss:0.1610\n",
            "epoch:261, loss:0.1580\n",
            "epoch:271, loss:0.1551\n",
            "epoch:281, loss:0.1523\n",
            "epoch:291, loss:0.1497\n",
            "epoch:301, loss:0.1472\n",
            "epoch:311, loss:0.1449\n",
            "epoch:321, loss:0.1427\n",
            "epoch:331, loss:0.1406\n",
            "epoch:341, loss:0.1386\n",
            "epoch:351, loss:0.1366\n",
            "epoch:361, loss:0.1348\n",
            "epoch:371, loss:0.1330\n",
            "epoch:381, loss:0.1313\n",
            "epoch:391, loss:0.1297\n",
            "epoch:401, loss:0.1282\n",
            "epoch:411, loss:0.1267\n",
            "epoch:421, loss:0.1253\n",
            "epoch:431, loss:0.1239\n",
            "epoch:441, loss:0.1226\n",
            "epoch:451, loss:0.1213\n",
            "epoch:461, loss:0.1201\n",
            "epoch:471, loss:0.1189\n",
            "epoch:481, loss:0.1177\n",
            "epoch:491, loss:0.1166\n",
            "epoch:501, loss:0.1155\n",
            "epoch:511, loss:0.1145\n",
            "epoch:521, loss:0.1135\n",
            "epoch:531, loss:0.1125\n",
            "epoch:541, loss:0.1116\n",
            "epoch:551, loss:0.1107\n",
            "epoch:561, loss:0.1098\n",
            "epoch:571, loss:0.1089\n",
            "epoch:581, loss:0.1081\n",
            "epoch:591, loss:0.1073\n",
            "epoch:601, loss:0.1065\n",
            "epoch:611, loss:0.1057\n",
            "epoch:621, loss:0.1050\n",
            "epoch:631, loss:0.1043\n",
            "epoch:641, loss:0.1036\n",
            "epoch:651, loss:0.1029\n",
            "epoch:661, loss:0.1022\n",
            "epoch:671, loss:0.1015\n",
            "epoch:681, loss:0.1009\n",
            "epoch:691, loss:0.1003\n",
            "epoch:701, loss:0.0997\n",
            "epoch:711, loss:0.0991\n",
            "epoch:721, loss:0.0985\n",
            "epoch:731, loss:0.0979\n",
            "epoch:741, loss:0.0974\n",
            "epoch:751, loss:0.0968\n",
            "epoch:761, loss:0.0963\n",
            "epoch:771, loss:0.0958\n",
            "epoch:781, loss:0.0953\n",
            "epoch:791, loss:0.0948\n",
            "epoch:801, loss:0.0943\n",
            "epoch:811, loss:0.0938\n",
            "epoch:821, loss:0.0933\n",
            "epoch:831, loss:0.0929\n",
            "epoch:841, loss:0.0924\n",
            "epoch:851, loss:0.0920\n",
            "epoch:861, loss:0.0915\n",
            "epoch:871, loss:0.0911\n",
            "epoch:881, loss:0.0907\n",
            "epoch:891, loss:0.0903\n",
            "epoch:901, loss:0.0899\n",
            "epoch:911, loss:0.0895\n",
            "epoch:921, loss:0.0891\n",
            "epoch:931, loss:0.0887\n",
            "epoch:941, loss:0.0884\n",
            "epoch:951, loss:0.0880\n",
            "epoch:961, loss:0.0876\n",
            "epoch:971, loss:0.0873\n",
            "epoch:981, loss:0.0869\n",
            "epoch:991, loss:0.0866\n",
            "accuracy: 0.9561\n"
          ]
        }
      ]
    }
  ]
}